{
  "slug": "sander-schulhoff-20",
  "quotes": [
    {
      "id": "sander-schulhoff-20-q001",
      "speaker": "Sander Schulhoff",
      "text": "AI guardrails are one of the more common defenses. And it's basically, for the most part, it's a large language model that is trained or prompted to look at inputs and outputs to an AI system and determine whether they are valid or malicious. What I have found through running these events is that they are terribly, terribly insecure and frankly, they don't work. They just don't work.",
      "timestamp": "00:07:34",
      "source": {
        "slug": "sander-schulhoff-20",
        "path": "episodes/sander-schulhoff-20/transcript.md",
        "lineStart": 91,
        "lineEnd": 91
      },
      "themes": ["ai-security", "guardrails", "adversarial-robustness"],
      "zones": ["chaos", "discovery"]
    },
    {
      "id": "sander-schulhoff-20-q002",
      "speaker": "Sander Schulhoff",
      "text": "The number of possible attacks against another LLM is equivalent to the number of possible prompts. Each possible prompt could be an attack. For a model like GPT-5, the number of possible attacks is one followed by a million zeros. It's basically infinite. When these guardrail providers say we catch everything, that's a complete lie.",
      "timestamp": "00:31:04",
      "source": {
        "slug": "sander-schulhoff-20",
        "path": "episodes/sander-schulhoff-20/transcript.md",
        "lineStart": 232,
        "lineEnd": 232
      },
      "themes": ["ai-security", "attack-surface", "guardrails"],
      "zones": ["chaos", "data"]
    },
    {
      "id": "sander-schulhoff-20-q003",
      "speaker": "Sander Schulhoff",
      "text": "We just released a major research paper alongside OpenAI, Google DeepMind, and Anthropic that took a bunch of adaptive attacks and human attackers and threw them at all the state-of-the-art models and defenses. We found that humans break everything. A hundred percent of the defenses in maybe like 10 to 30 attempts.",
      "timestamp": "00:33:25",
      "source": {
        "slug": "sander-schulhoff-20",
        "path": "episodes/sander-schulhoff-20/transcript.md",
        "lineStart": 238,
        "lineEnd": 238
      },
      "themes": ["red-teaming", "human-attackers", "research"],
      "zones": ["discovery", "data"]
    },
    {
      "id": "sander-schulhoff-20-q004",
      "speaker": "Sander Schulhoff",
      "text": "If the smartest AI researchers in the world can't solve this problem, why do you think some random enterprise who doesn't really even employ AI researchers can? It just doesn't add up. And another question you might ask yourself is, they applied their automated red teamer to your language models and found attacks that worked. What happens if they apply it to their own guardrail?",
      "timestamp": "00:37:02",
      "source": {
        "slug": "sander-schulhoff-20",
        "path": "episodes/sander-schulhoff-20/transcript.md",
        "lineStart": 247,
        "lineEnd": 247
      },
      "themes": ["ai-security", "industry-critique", "guardrails"],
      "zones": ["chaos", "intuition"]
    },
    {
      "id": "sander-schulhoff-20-q005",
      "speaker": "Sander Schulhoff",
      "text": "You can patch a bug, but you can't patch a brain. If you find some bug in your software and you go and patch it, you can be 99.99% sure that bug is solved. If you go and try to do that in your AI system, you can be 99.99% sure that the problem is still there. It's basically impossible to solve.",
      "timestamp": "00:40:49",
      "source": {
        "slug": "sander-schulhoff-20",
        "path": "episodes/sander-schulhoff-20/transcript.md",
        "lineStart": 262,
        "lineEnd": 265
      },
      "themes": ["ai-vs-classical-security", "adversarial-robustness", "mental-model"],
      "zones": ["chaos", "discovery"]
    },
    {
      "id": "sander-schulhoff-20-q006",
      "speaker": "Sander Schulhoff",
      "text": "Prompt-based defenses are the worst of the worst defenses. We've known this since early 2023. There have been various papers out on it. We've studied it in many, many competitions. They don't work. Even more than guardrails, they really don't work, like a really, really, really bad way of defending.",
      "timestamp": "00:42:57",
      "source": {
        "slug": "sander-schulhoff-20",
        "path": "episodes/sander-schulhoff-20/transcript.md",
        "lineStart": 268,
        "lineEnd": 268
      },
      "themes": ["prompt-engineering", "defenses", "ai-security"],
      "zones": ["data", "chaos"]
    },
    {
      "id": "sander-schulhoff-20-q007",
      "speaker": "Sander Schulhoff",
      "text": "Any data that AI has access to, the user can make it leak it. Any actions that it can possibly take, the user can make it take. So make sure to have those things locked down. This brings us to classical cybersecurity, like proper permissioning. Where classical cybersecurity and AI security meet, that's where the important stuff occurs.",
      "timestamp": "00:48:49",
      "source": {
        "slug": "sander-schulhoff-20",
        "path": "episodes/sander-schulhoff-20/transcript.md",
        "lineStart": 301,
        "lineEnd": 304
      },
      "themes": ["permissioning", "cybersecurity", "ai-security"],
      "zones": ["focus", "perfection"]
    },
    {
      "id": "sander-schulhoff-20-q008",
      "speaker": "Sander Schulhoff",
      "text": "Somebody who's at the intersection of AI security and cybersecurity would look at the system and say, 'Hey, this AI could write any possible output. Some user could trick it into outputting anything. What's the worst that could happen?' They'd realize we can just dockerize that code run, put it in a container, and now we're completely secure. Prompt injection, completely solved.",
      "timestamp": "00:52:54",
      "source": {
        "slug": "sander-schulhoff-20",
        "path": "episodes/sander-schulhoff-20/transcript.md",
        "lineStart": 319,
        "lineEnd": 322
      },
      "themes": ["containment", "architecture", "problem-solving"],
      "zones": ["focus", "perfection", "velocity"]
    },
    {
      "id": "sander-schulhoff-20-q009",
      "speaker": "Sander Schulhoff",
      "text": "AI researchers are the only people who can solve this stuff long-term, but cybersecurity professionals are the only ones who can kind of solve it short term, largely in making sure we deploy properly permissioned systems and nothing that could possibly do something very, very bad. That confluence of career paths is going to be really, really important.",
      "timestamp": "00:58:34",
      "source": {
        "slug": "sander-schulhoff-20",
        "path": "episodes/sander-schulhoff-20/transcript.md",
        "lineStart": 349,
        "lineEnd": 349
      },
      "themes": ["career-paths", "cybersecurity", "ai-research"],
      "zones": ["alignment", "focus"]
    },
    {
      "id": "sander-schulhoff-20-q010",
      "speaker": "Sander Schulhoff",
      "text": "It's easier to tell the model 'Never do this' than with emails and stuff 'Sometimes do this.' With CBRN you can be like 'Never, ever talk about how to build a bomb.' But with sending an email, you have to be like 'Hey, definitely help out send emails, oh, but unless there's something weird going on, then don't send email.' It's a much more difficult problem.",
      "timestamp": "01:15:35",
      "source": {
        "slug": "sander-schulhoff-20",
        "path": "episodes/sander-schulhoff-20/transcript.md",
        "lineStart": 490,
        "lineEnd": 493
      },
      "themes": ["agentic-security", "permissions", "nuanced-rules"],
      "zones": ["chaos", "focus"]
    },
    {
      "id": "sander-schulhoff-20-q011",
      "speaker": "Sander Schulhoff",
      "text": "We're finally in a situation where the systems are powerful enough to cause real world harms. And I think we'll start to see those real world harms in the next year. With image classifiers, nobody went through the effort of placing tape on the stop sign to trick the self-driving car. But what we're starting to see with LLM powered agents is that they can be tricked and we can immediately see the consequences.",
      "timestamp": "01:24:23",
      "source": {
        "slug": "sander-schulhoff-20",
        "path": "episodes/sander-schulhoff-20/transcript.md",
        "lineStart": 523,
        "lineEnd": 523
      },
      "themes": ["real-world-harm", "agents", "predictions"],
      "zones": ["chaos", "velocity"]
    },
    {
      "id": "sander-schulhoff-20-q012",
      "speaker": "Sander Schulhoff",
      "text": "Guardrails don't work, they really don't work. And they're quite likely to make you overconfident in your security posture, which is a really big problem. Stuff's about to get dangerous. We're starting to see agents deployed, robotics deployed that are powered by LLMs, and this can do damage. It can cause financial loss, eventually physically injure people.",
      "timestamp": "01:27:54",
      "source": {
        "slug": "sander-schulhoff-20",
        "path": "episodes/sander-schulhoff-20/transcript.md",
        "lineStart": 535,
        "lineEnd": 538
      },
      "themes": ["guardrails", "overconfidence", "real-world-harm", "agents"],
      "zones": ["chaos", "focus", "discovery"]
    }
  ],
  "themes": ["ai-security", "adversarial-robustness", "guardrails", "prompt-injection", "cybersecurity", "agentic-risk", "permissioning", "red-teaming"],
  "takeaways": [
    "AI guardrails fundamentally do not work: the attack space is effectively infinite, human attackers break 100% of defenses in 10-30 attempts, and no guardrail provider can claim statistically significant protection.",
    "You can patch a bug but you can't patch a brain: AI security is fundamentally different from classical cybersecurity because fixing one vulnerability in an LLM gives you essentially zero confidence the underlying problem is resolved.",
    "The real solution lies at the intersection of classical cybersecurity and AI security: proper data permissioning, action containment via techniques like CAMEL, and dockerized execution environments can completely neutralize certain attack vectors.",
    "As AI systems gain agentic capabilities, the stakes escalate dramatically: robots can be prompt-injected into physical harm, email agents can be tricked into exfiltrating data, and browser agents can leak user secrets from visited webpages.",
    "Education and awareness are the most impactful defenses today: understanding that any data an AI can access can be leaked, and any action it can take can be forced, should drive every deployment decision."
  ],
  "zone_influence": {
    "velocity": 0.05,
    "perfection": 0.10,
    "discovery": 0.15,
    "data": 0.10,
    "intuition": 0.05,
    "alignment": 0.05,
    "chaos": 0.35,
    "focus": 0.15
  },
  "contrarian_candidates": [
    {
      "quoteId": "sander-schulhoff-20-q004",
      "why": "Directly challenges the entire AI security guardrails industry as fundamentally broken, arguing that enterprises paying for these solutions are wasting money on products that their own vendors' tools would also break",
      "related_zones": ["chaos", "intuition"]
    },
    {
      "quoteId": "sander-schulhoff-20-q005",
      "why": "The 'can't patch a brain' metaphor is a paradigm-breaking insight that undermines the core assumption of iterative security fixes that the entire software industry relies on",
      "related_zones": ["chaos", "discovery"]
    },
    {
      "quoteId": "sander-schulhoff-20-q008",
      "why": "Argues that containment via classical architecture (dockerization) can completely solve prompt injection in certain cases, which is counterintuitive when the framing is that nothing works",
      "related_zones": ["focus", "perfection"]
    }
  ],
  "guest_metadata": {
    "guest_type": "academic",
    "company_stage": "seed",
    "primary_topics": ["ai-security", "adversarial-robustness", "red-teaming"]
  }
}